% !TEX root = template.tex

\maintitle{The Changing Role of Statisticians and their Software}

\section{The rise of data}

%the rise of statistical computing
The field of computational statistics finds itself in a transition that is bringing tremendous opportunities but also challenging some of its foundations. Historically, statistics has been an important early application of computer science, with numerical methods for analyzing data dating back to the 1960s. The discipline has evolved largely on its own terms and has a proud history with pioneers such as John Tukey using some of the first programmable machines at Bell Labs for exploratory data analysis \citep{brillinger2002john}. Well known techniques such as the \emph{Cooley-Tukey FFT algorithm} \citep{cooley1965algorithm} and the \emph{box-plot} \citep{tukey1977exploratory} as well as computer terminology such as \emph{bit} and \emph{software} were conceived at this time. These visionary contributions to the process of computing on data laid the foundations for current practices in data analysis. Computer programs developed at Bell Labs in the 1970s and 1980s, such as \texttt{UNIX} \citep{ritchie1974unix} and \texttt{S} \citep{becker1984s, becker1988new} have strongly influenced statistical software as we know it today. In fact, the \texttt{R} environment \citep{R}, which features prominently in this thesis, is often dubbed the lingua franca of statistics and is a direct ancestor of the \texttt{S} language as originally proposed by John Chambers. In the traditions of Tukey and Bell Labs, the field has made remarkable progress over the past few decades. Specifically the \texttt{S} language has, to quote the Association for Computing Machinery, \emph{``forever altered how people analyze, visualize, and manipulate data''} \citep{chambersaward}. Largely due to the availability of statistical software, data analysis has become the de facto method of empirical sciences, an integral part of academic curricula, and plays an increasingly prominent role in modern society.

%the change
However, half a century after Tukey first demonstrated how to compute the Fourier transform on a machine, big changes are on the horizon. 
Modern society has put data at the frontier for innovation and productivity, presenting problems of a unprecedented scale and complexity \citep{manyika2011big}.
Developments in computer technology and scientific practices ask for more flexible and scalable analysis tools. 
%Increasing who's pressure to preform? Statistics? The software? It is not clear what you are trying to say.
At the same time, fast growing adjacent fields such as artificial intelligence, software engineering and graphic design are increasingly touching and overlapping with statistics. 
%the future
The combination and interaction of these factors is blurring the borders of traditional disciplines. Techniques for collection, management, and analysis of data have started to melt into a hybrid field of data. This joint discipline is enormous, which leads to the emergence of many new sub-disciplines based on domain knowledge and technical expertise.
%Data-science, data-engineering and data-journalism are some examples of professions that were unheard of five years ago. 
A proliferation of short-lived ideas, paradigms, vocabulary, software, and business models has accompanied this tumultuous transition phase. The upcoming years will likely be decisive in determining where this transition converges, who takes the lead, and which branches are destined to become extinct.
%This paragraph is great and it sets you up to talk about your research. If I was you I would use this as a spring board to give the reader a taste of your research.


%haven't really mentioned the problems yet
%These trends might sound worrisome, but it does not mean that statistics as we know it is about to disappear. On the contrary. 
It is in the interest of both the statistician and the scientific community that the statistical tradition and theoretical foundations do not get diluted in this transition. The experience and understanding of learning from data that exists in the literature and minds of the statistical community harbors unique value. Statistical principles are strongly rooted in probability theory and methods that have evolved over the years are best of breed and proven to be effective. Only statisticians truly master the fine art of carefully exploring and testing data in order to unveil relations, patterns, and stories hidden behind the numbers. This holistic approach of studying each individual dataset in all its facets is not very well understood by any other community. Moreover, the infrastructure and expertise in research and development of statistical methods will become only more valuable as the field expands. This wisdom and maturity provides the credentials for a leading role in the new era of data. Our discipline enjoys great respect from other fields and we are invited to show the way. 
%However, if we decline our new to role as team player, the boat might leave without us. 
However, we must also face our limitations. Modern data analysis involves technical challenges that are beyond our capabilities and require interdisciplinary collaboration. This changing reality asks for some reflection on what is the true strength of statistics and which tasks are perhaps better suited for other experts.


\subsection{Size and scale}

Amongst the most visible recent developments are those related to scale. The rapid growth of data and the rise of computationally intensive methods have multiplied demand for computational power. Over the past years, estimation based on resampling or Monte Carlo techniques %\citep{efron1982jackknife}% 
has steadily gained popularity over analytical solutions. 
These new methods are often easier to develop and require less assumptions of the data, at the cost of computational energy.
Therefore, many universities have invested in supercomputers that students and faculty share to schedule such computationally intensive processing jobs. More recently, a new class of problems under the umbrella term of \emph{big data} has entered the arena. These are problems which by definition call for quantities of memory and cpu that a single machine can not possibly provide. Big data analysis needs to be spread out on computing clusters, which requires fundamental redesigns of analytical software to support distributed computing. Algorithms need to be based on naturally parallelizable design patterns such as \emph{map-reduce} \citep{dean2004mapreduce} in order to generalize to large scale data processing. 

It has become painfully clear that these challenges are beyond the domain of the traditional statistical software. The big data movement has been driven by the IT industry with a strong emphasis on software engineering solutions, such as distributed file-systems and tools for managing computing resources in clusters. Statistical products provide an interface to such systems at best. Big data software is still in its infancy and analysis programs that build on these stacks are quite rudimentary in comparison with mainstream statistical software. However, demand from governments and industries has made big data into big business, resulting in enormous efforts to close this gap. We are starting to see the first open source products 
%such as H$_2$0
that layer on Hadoop to implement natively distributed versions of statistical methods such as GLM, PCA and K-Means. Over the upcoming years, these systems will likely start replacing current software to provide better support for big data and distributed computing. 
Tools and techniques for data analysis originating from fields other than statistics are received with a mix of excitement, skepticism, and aversion that is dividing the community. However, it is absolutely critical to the survival of our discipline that statisticians join the big data movement and work with these new players to influence the next generation of analysis software.


\subsection{Visualization}

Alongside quantitative results such as averages and parameter estimates, another important device for inspecting data is provided by computer graphics. Statistical software packages typically include functionality to generate plots in some form or another. Implementations vary anywhere from low-level shape drawing to completely canned charts automatically included with particular analyses. For many years such graphing tools have provided a useful and popular complement to numerical methods in statistical software products. However visualization too has matured into its own field independent of statistical computing. Specialized programs and libraries have started to offer more powerful, flexible visualization capabilities than those found in statistical software, slowly taking away market share. 

The definitive tipping point has been the introduction of advanced graphing capabilities \texttt{SVG}, \texttt{Canvas} and \texttt{WebGL} in web browsers as part of the \texttt{HTML5} standard in 2012. These technologies opened the door to high performance, vector based, interactive, and even \texttt{3D} visualization in web pages. They take advantage of dedicated languages for styling (\texttt{CSS}) and interaction (\texttt{JavaScript}) while leveraging a user base that literally includes the entire world. In very little time countless high quality open source \texttt{JavaScript} libraries have appeared that go far beyond traditional graphics. By taking advantage of internet access and the document object model (\texttt{DOM}), browser graphics introduce a new generation of visualizations including interactive maps, live data animation and data-driven documents \citep{bostock2011d3}. But even for implementing traditional static plots, the browser offers unprecedented flexibility and performance and which eventually replace other graphics devices.

\subsection{Domain specialization}

The rise of data and statistics has given birth to many new specializations and sub-fields. Over the past two decades much of the social sciences, including psychology and political science have transitioned from mostly qualitative towards quantitative methods, making measurement and analysis of behavior the main subject of research. Other applied branches like biostatistics and econometrics have long outgrown their respective disciplines and are supporting dedicated degrees and departments. In bioinformatics, entire schools are forming around various types of genomic data, such as DNA microarrays and \texttt{ChIP}-sequencing. More recently, even areas like literature and journalism are discovering data as a source of information. 
These fields encounter data and problems of a different nature, but have already unveiled very promising applications. 
Following progress in academia, increasingly many organizations in the public and private sector are embracing statistical methods as well. Retailers rely on data analysis to predict sales, target advertisement, and improve recruiting, whereas governments use it to study demographics, evaluate policy, and mass surveillance to name a few applications. 

An important consequence of these developments is that analysts and their tools become increasingly specialized. Whereas statistics used to be practiced mainly in universities and research labs, data analysis is now part of job descriptions in all corners of the economy. This transition shifts the emphasis of analysis tools away from general purpose software towards applications tailored specifically to data or problems as they appear in a particular field or occupation. To develop such tools, the statistician must work in a team of engineers, user interface designers and domain experts to implement custom applications with embedded analysis methods. This asks for a more flexible approach to statistical software which allows for incorporating domain knowledge and third party software in order to cater to the demands of specific user groups. 
 


\subsection{Socializing data analysis}

A somewhat more cultural yet important trend in recent years has been the socializing of technology. Much of software innovation has shifted focus from optimizing ways in which users perform particular tasks towards improving communication and collaboration between people. One popular example is Github: a software hosting service with social networking features based on the revision control system \texttt{git} \citep{torvalds2010git}. Its excellent branching and merging tools enable distributed non-linear development by making it effortless to fork, prototype, contribute, review, discuss, and incorporate code between users and projects. This system has revolutionized code management for open-source projects and has proven to catalyze efficiency and creativity in the  development process. Many of the younger statistical software developers are already using Github or similar platforms to host and manage their scripts and packages. 

There is a lot of room for further socializing the data analysis process itself as well, especially within the sciences. Practices in statistics have changed surprisingly little over the past few decades. Statisticians usually work with a locally installed software product to manipulate and analyze data, and then copy results into a report or presentation. Collaboration is often limited to sharing a script or data file on a personal homepage. Many have argued that modernizing these practices is long overdue and vital to the future of statistics. According to \cite{future2013} academic publication is on an irreversible trajectory moving in the direction of online, dynamic, open-access publishing models. Changing demands for reproducibility \citep{peng2011reproducible} and teaching \citep{nolan2010computing} are also pushing for more transparent and accessible analysis tools. This coincides with the global movement of universities, governments and industries towards open data, open access knowledge and open source software. With the availability of enabling technologies, these developments suggest a future where reproducible data, code and results become an integral part of the scientific publication process. Internet based data analysis platforms with social networking features could greatly advance collaboration, peer review and education of statistics while at the same time addressing data management and scalability issues.



\subsection{Integration and interoperability}

In my opinion, all of these developments highlight the urgency of interdisciplinary collaboration and integration for the future of our field. Modern society presents problems that can only be tackled through joint effort of statisticians, engineers, computer scientists, web developers, graphic designers and domain experts. This requires statisticians to surrender some of their independence and learn to become better team players. Finding our new identity in this dynamic is perhaps the main challenge for the current generation of statisticians. However, doing everything ourselves is no longer feasible. Failure to adapt to this new role puts us at serious risk of becoming isolated and obsolete.

I am particularly concerned with the changing role of statistical software. It is my belief that \emph{interoperability} of statistical tools with other software will be the key factor to sustained pertinence of the discipline. Today, statistical products are primarily designed as do-it-all standalone suites serving every need of the independent statistician. The technology and culture of statistical computing has been formed around the assumption that we are at the end of the software food chain. Statistical software packages make it easy to call out to a database, \texttt{C++} library or web \texttt{API}, but little thought and effort is put into interfacing statistical methods from third party software. Unfortunately this dynamic is not unlike the attitude of many statisticians themselves. In order to leverage statistical methods in systems, pipelines, and applications, the focus needs to shift away from standalone products for statisticians, towards modular analysis units. Rather than competing with big data software or browser based visualization, we should implement statistical methods that can integrate with these upcoming technologies. Tools that play nice with other software can facilitate better collaboration between statisticians and other team members and smoothen the transition into the interdisciplinary world. The demand for high quality analysis tools that the statistics community has to offer is enormous, but making them widely applicable requires some changes in the way we design our software. This brings us to the theme of this thesis: \emph{embedded scientific computing}. 

%Finding our new role in this dynamic and becoming better team players is perhaps the main challenge for the current generation of statisticians. However, failure to adapt to this new role puts statisticians at risk of becoming isolated and obsolete.

%The will need to work with engineers, computer scientists, web developers, graphic designers and domain experts to  to tackle a new class of problems.


% All the chanllenges address so far highlight the increased need for statistician to coolaborte with other disciplines and sectors beyond academia to provide specialized data analysis tools. In order to accomplish this we need look inwards and find our identity. The statistical software that we currently have available (such as blah blah blha) have steep learning curves and usually includes functionalitles that are not directly related to statistics. Our quest, as a field, to address the growing need of modern society to analyze data has driven us outside our expertise and into unfamiliar waters. This quest has dilluted the field to the point that we are doing things that other fields are doing better and more efficiently. Rather than trying to compete, the community should go back to its roots and actively engage with other players to create interoperable software that focuses solely on statistical methods. This brings us to the theme of this thesis: \emph{embedded scientific computing}. 

\section{Motivation and scope}

In 2009 I co-authored an article for the journal \emph{Statistics in Medicine} \citep{van2009stage} which proposed a novel statistical method for diagnosing developmental disorders in young teenagers. Along with the paper and \texttt{R} implementation, we developed a free web service to directly use this method online without the need for technical knowledge or specialized software. The application was relatively primitive but succeeded in making a state of the art diagnosis tool widely available to clinics and hospitals in The Netherlands. It enabled physicians and pediatricians with limited experience in statistics or \texttt{R} to immediately take advantage of recent statistical innovation. For me, this was a proof of concept that showed the enormous potential of integrating open source analysis software in specialized applications. Scaling this up could greatly improve accessibility of statistical methods while multiplying return on investment of our efforts. In the period leading up to and during my graduate studies at UCLA, I have been involved in many similar projects developing software with embedded analysis and visualization, both in academic and commercial organizations. These included systems, web applications and reporting tools in areas such as health, education, geography, and finance. Experiences and struggles from such diverse projects provided unique insights in the recurring challenges of embedded scientific computing.% and were an inspiration to this research. 
% By identifying and addressing the core problems in this area I hope we can unlock the power of statistics in the next generation of software.

The topic of embedded scientific computing has not previously been studied with the attention and detail it deserves. The problems are underdetermined and their complexity is widely underestimated. Over the course of my studies I have come to realize that the true nature of the problems is not only caused by technical limitations, but has to be understood from a disconnect between disciplines in an interdisciplinary space. The subject intersects statistical computing, software engineering and applied data analysis, however insufficient overlap and joint work between researchers in these fields is impeding integration of tools and knowledge. Many technical problems arise due to lack of understanding of the practices in scientific computing and the type of problems that it encounters. Underappreciation of the domain logic and inner workings of statistical software has resulted in solutions that are impractical, unreliable or unscalable. Therefore, this dissertation starts with an in-depth exploration of what scientific computing is about and how it is different from other applications of computer science. By mapping out the domain logic and identifying important bottlenecks, I hope my work can contribute towards aligning the various efforts in this space and lead to a more coherent set of tools and knowledge for developing integrated analysis components. 

The motivation and approach for this research were entirely bottom-up. The intention was never to solely apply or extend a particular existing technology or theoretical framework. Instead I studied the challenges and solutions as they arise in practice when implementing systems and applications with embedded analysis and visualization. From these experiences I tried to distill the principal problems in order to develop a general purpose software system. The process involves an empirical back and forth between diagnosing and refining problems, while experimenting with various approaches for solutions. This contrasts with most current research in statistics where implementation details are often considered an afterthought. In this thesis the software itself is the subject of the study and the conclusions largely follow from implementation rather than the other way around. Much effort has gone into developing high quality software to put various approaches to the test. The results are a convergence of many iterations of prototyping, testing, refactoring, and incorporating feedback. 


\subsection{Definition}

The purpose of this research is to identify and discuss fundamental challenges of \emph{scaling embedded scientific computing}. \emph{Scientific computing} refers to the general class of computational methods for data manipulation, statistics, mining, algorithm development, analysis, visualization, and related functionality. Unfortunately there is no universally agreed-upon umbrella term for this genre of data-driven research software. Vocabulary and scope of the countless packages vary by domain and are constantly evolving. For example, \texttt{R} is officially a ``software environment for statistical computing and graphics'', but this does not exactly capture functionality for scraping, text processing, meta programming, and web development, to name some recent additions. Similarly, \texttt{Matlab} is marketed as a ``numerical computing environment'', \texttt{Julia} is ``a programming language for technical computing'', and \texttt{NumPy} is ``the fundamental package for scientific computing with \texttt{Python}''. In addition, many more specialized software packages use jargon specific to particular applications or domains. Each of these packages takes a unique angle, but they share a similar purpose and overlapping audience. Perhaps the most distinguishing characteristic is the emphasis of \emph{data as objects} combined with a functional style of programming that resembles mathematical operations. No single description does justice to this continuously expanding body of methods and software. Within this thesis the various terms are used mostly interchangeably with minor differences in emphasis. But for the title \emph{scientific computing} seems like the most general neutral term for this branch of computing.

The term \emph{embedded} deserves some clarification as well. In the context of this thesis, it does not refer to software specifically written for a particular electronic or mechanical device, which is the conventional meaning of embedded systems in computer science. Instead, the term has been adopted from the \texttt{R} community. For example \cite{neuwirth2001embedding} talked about ``Embedding \texttt{R} in standard software'' and \cite{eddelbuettel2011rcpp} state that ``The \texttt{RInside} package provides \texttt{C++} classes that make it easier to embed \texttt{R} in \texttt{C++} code''. Moreover, the \emph{Writing R Extensions} manual mentions the term frequently in the chapter ``Embedding \texttt{R} under Unix-alikes'' \citep{writingextensions}. I use \emph{embedded} to emphasize the role of software as a \emph{component}, in contrast with a standalone application. In software engineering, a component is a module or web resource that encapsulates a set of related functions or data \citep{cox1990planning}. It builds on the principle of separation of concerns and advocates a reuse-based approach to defining, implementing and composing loosely coupled independent modules into systems. Components are generally not interfaced by the user, but rather called from another piece of software through a programmable interface. %mention packages already component-ish%?

Finally, \emph{scalability} connotes the ability of a system to accommodate an increasing number of elements or objects, to process growing volumes of work gracefully and be susceptible to enlargement \citep{bondi2000characteristics}. Commercial vendors of statistical software often present scale exclusively in relation to the magnitude of the data and offer solutions to process more data in less time. Although important, there are many additional dimensions and directions to scale other than memory and disk space. For example, accommodating large amounts of users or concurrent tasks introduces new management and organization problems. Also systems that are high maintenance and require considerable human resources to accommodate growth do not scale well by definition. Furthermore, increased code complexity in large projects often reveals difficulties which were invisible on a smaller scale. Finally an important observation is that in order to scale up, systems must be able to scale down. Software which requires a special infrastructure and a team of experts and administrators to operate has limited applications, regardless of performance. On the other hand, a solution that has the potential to scale up to large problems, but also works for small projects by users with limited expertise is much more likely to find wide adoption. Human dimensions to scalability are easily overlooked, but in practice just as critical as overcoming technical limitations.

\subsection{Overview}

The main body of the thesis consists of four contributions which form the corner stones of the research. These pieces treat the most pressing and difficult problems that are encountered when building systems with embedded scientific computing. Chapter 2 presents the central work of this thesis and treats the problem of interfacing computational procedures. Currently most statistical software is designed for \texttt{UI} rather than \texttt{API} interaction. However, clients or systems integrating analytical components require a programmable interface that exposes desired functionality while abstracting implementation details. The principle of separation of concerns requires us to carefully carve out logic specific to scientific computing and distinguish it from any application or implementation logic. To this end, the majority of the chapter is concerned with a high level description of the principles and practices of scientific computing. The \texttt{OpenCPU} system is introduced as a proof of concept and reference implementation that puts the advocated approach in practice. Experiences from developing and applying the \texttt{OpenCPU} software are a driving factor throughout the research and also motivate the remaining chapters of this thesis. 

The subsequent chapters treat more technical issues that come up with embedded scientific computing. Chapter 3 talks about enforcing security policies and managing hardware resources. Because statistical software products are traditionally designed for local use, access control is generally not considered an issue and the execution environment is entirely unrestricted. However, embedded statistical components require fine grained control over permissions and resources allocated to individual users or tasks. The chapter discusses various security models and explains the domain specific aspects of the problem that make it difficult to apply general purpose solutions. The \texttt{RAppArmor} package is introduced to illustrate \emph{mandatory access control} style security in \texttt{R} on \texttt{Linux} systems. This allows for divorcing the security concern from the application layer and solve it on the level of the operating system. \texttt{RAppArmor} is at the basis of the \texttt{OpenCPU} cloud server implementation and makes it possible to expose arbitrary code execution privileges to the public. This provides a basis for applications where users can freely share, modify and run custom code which is central to the \texttt{OpenCPU} philosophy.

Chapter 4 discusses the issue of data interchange, specifically in the context of the \texttt{JSON} format. A key challenge for interfacing statistical components is getting data in and out of the system. To this end, \texttt{OpenCPU} supports two popular data interchange formats: \texttt{JSON} and \texttt{Protocol Buffers} \citep{rprotobuf}. However the main difficulties are not so much related to the format, but rather to the \emph{structure} of input and output data. Strong typed languages typically use schemas to formally describe and enforce structure. However this is not very natural for dynamically typed languages with loosely defined data structures frequently used in scientific computing. Instead we take an approach that directly maps the most important data types in \texttt{R} to \texttt{JSON} and vice versa. This allows clients to interact with \texttt{R} functions and objects via \texttt{JSON} without any specific knowledge about implementation of object classes in \texttt{R}. The \texttt{jsonlite} package is used as a reference implementation throughout the chapter to illustrate this mapping. Besides \texttt{OpenCPU}, at least a dozen other projects have already adopted the \texttt{jsonlite} package to interact with \texttt{JSON} data in \texttt{R} as well.

Finally, chapter 5 treats the more high level issue of managing software versioning and specifically inter-software dependencies in the context of scientific computing. In my experience, imprudent software versioning is by far the primary cause of problems in statistical software. Rapidly growing repositories have made current practices unsustainable and properly addressing this issue is critical for turning statistical methods into reliable components and applications. Unfortunately the problem is not sufficiently understood and acknowledged by most members of the community. This can be traced back to a long culture of exclusively interactive use which assumes that the user will manually manage software and debug problems as they appear. The chapter explores the problem in great detail and explains how limitations of dependency versioning are causing unstable software and irreproducible results. I make a case to the \texttt{R} community for moving to a design that better accommodates modern software development and can scale to large repositories.  

%\subsection{Less is more}

%\begin{quote}
%\emph{``perfection is reached not when there is nothing left to add, but when there is nothing left to take away''} --- Antoine de Saint-Exupéry
%\end{quote}


%KISS (\emph{``Keep it simple, stupid''}) is a popular ancronym coined in the 1960s by the U.S. Navy \citep{victor2007concise} for the design principle stating that systems work best if they are kept simple rather than made complicated. Some other frequently cited testimonials of similar observations include \emph{``less is more''} (Mies van der Rohe), \emph{``Simplicity is the ultimate sophistication''} (Leonardo da Vinci), and \emph{``perfection is reached not when there is nothing left to add, but when there is nothing left to take away''} (Antoine de Saint-Exupéry). 

%\noindent Management of complexity is one of the most challenging aspects of modern software development. When pressure and temptation to expand functionality is not balanced by a constant strive for simplicity, a project is heading for failure. Each newly introduced feature demands additional maintenance and support down the road, which can rapidly turn into a sinkhole of developer time. Moreover overhead and limitations introduced by unwanted stuff make bloated software less attractive. Also programs gets increasingly difficult to learn and understand if important principles are masked by bells and whistles of vaguely related features. Therefore finding a minimal generalizable solution to address a particular problem or market is key to software simplicity. However this is rarely easy. It requires careful deliberation of the scope and priorities of a system; of what it \emph{is} and what it should or should not do. Incomplete solutions that leave important problems unsolved are unreliable and impractical. However components that are overly involved might conflict with other pieces, and complicate the system. The quest for a clean and elegant design often consists of many iterations of adding, removing, abstracting, rethinking and refactoring the system.

%A lot of thought in this research has gone into finding the core challenges of embedded scientific computing. Rather than merely presenting software, each chapter in this thesis describes experiences, examples and use cases that illustrate how various issues are symptons of a fundamental underlying problem. I hope that identifying these problems the merit of this research is just as much that the merit of this research is 

\subsection{About R}

This research relies heavily on the \emph{The R Project for Statistical Computing}, for short: \texttt{R} \citep{R}. The \texttt{R} language is currently the most popular open source software for statistical computing and considered by many statisticians as the de facto standard for data analysis. The software is well established and the huge \texttt{R} community provides a wealth of contributed packages in a variety of fields. Currently only \texttt{R} has the required maturity and infrastructure to build and test scalable computing systems. Therefore, \texttt{R} is the obvious candidate to experiment with embedded scientific computing. Most of my experiences that led to this research have been with \texttt{R}, and the software presented in this thesis is implemented on \texttt{R}. However, I want to emphasize that the purpose of this research was not just to develop software and the problems discussed are not limited to any particular implementation. Difficulties related to interfacing, security, data interchange and dependency management will appear in some form or another when implementing analysis components in any language. None of the currently available systems, including commercial products, provide any simple and general solutions.

Yet in reality it is very difficult to study or discuss software without an actual implementation. Only by developing and using software we discover the strengths and limitations of certain technology and learn which solutions are practical and reliable. For these reasons, the problems of embedded scientific computing are mostly treated from the way they take shape in \texttt{R}. Rather than presenting abstract ideas in general terms, we take advantage of the lingua franca to demonstrate problems and solutions as they appear in practice. Therefore each chapter discusses some relevant aspects of the \texttt{R} language that provide the context for a particular issue. For example, the conventional container format for namespacing a collection of data and functions in \texttt{R} is a \emph{package}. Even though other products might use different mechanics and jargon for combining and publishing a set of objects, any system needs some notion of namespaces and dependencies analogous to packages in \texttt{R}. Throughout the thesis I discuss concepts using terminology and examples from the \texttt{R} language, but repeatedly emphasize that software serves primarily as illustration. Similar techniques used in the \texttt{R} packages should work when building analysis components with \texttt{Julia}, \texttt{Matlab} or \texttt{Python}. 

%\section{Acknowledgements}

%This research would not have been possible without the help of many people. I would like to thank my adviser Mark Hansen and other committee members Jan de Leeuw, Deborah Estrin and Mark Handcock for giving me the confidence, support and liberty to pursue this somewhat unconventional interdisciplinary research. Their blessing and advice provided the perfect balance between guidance and creative freedom that allowed me to get the most out ouf my doctoral research. I thank Jan specifically for welcoming me at UCLA and personally making my California adventure possible. I am also grateful to my colleagues at Mobilize and OpenMHealth. Collaborating with these professional software developers was a big inspiration to my research and I thoroughly enjoy working with each of the individual team members. Finally a warm hug goes to our student affairs officer Glenda Jones for going above and beyond her duties to take care of the statistics students at UCLA. 

%Then there are the countless members of the \texttt{R} community that generously provided valuable support, advice, feedback and criticism. In particular, much of the initial research builds on pioneering work and advice from Jeffrey Horner. Expertise from Dirk Eddelbuettel and Michael Rutter provided important foundations for developing software systems with \texttt{R} on Debian and Ubuntu systems. The RStudio folks, including Hadley Wickham, JJ Allaire, Joe Cheng and Yihui Xie have given lots of helpful technical support. Finally I would like to thank the many individuals and organizations that adopted the \texttt{OpenCPU} software, especially in the early stages. By investing their time and energy in my work they were an important source of feedback and motivation to keep improving the software. 
